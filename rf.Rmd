---
title: "Titanic"
author: "Tal Barami"
date: "24 בנובמבר 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Set working directory:
```{r}
knitr::opts_knit$set(root.dir = 'C:\\Users\\Tal\\Desktop\\Titanic\\Titanic')
getwd()
```

Read the train.csv file into a dataframe. 
```{r}
df <- read.csv("Titanic/train.csv",na.strings = "")
```


Check the datatypes of the attributes using the *str* method. 
```{r}
str(df)
```

Convert numeric features into factors.
```{r}
df$Survived<- as.factor(df$Survived)
df$Pclass<- as.factor(df$Pclass)
```

Produce some descriptive statistics using the *Summary* method, and read it to get to know your data. Do you have missing data(NA) inside? If so, you should either preprocess the data to have non NAs, or use only models that can handle missing values. We will go with the second option, so you don't need to preprocess the data for now.
```{r}
summary(df)
```

It seems that the following features have too many categories or simply represent an index: PassengerId, Ticket, Name. We'll ignore these features.
```{r}
df <- df[,-c(1,4,9, 11)]
```


Divide the features' names to numerics and factors:
```{r}
cols<- 1:dim(df)[2]
factors <- cols[sapply(df,is.factor)]
numerics <- cols[!sapply(df,is.factor)]
```

```{r}
summary(df)
```

Convert our data to a numeric form,
```{r}
df.numeric <- df
df.numeric$Survived=as.numeric(df[,'Survived'] == "1")
```

Use CARET's *dummyVars* to convert nominal features to various dummies or flags, according to each feature's possible levels.
```{r}
library(caret)
dmy <- dummyVars(" ~ .", data = df.numeric,fullRank=F)
df.numeric <- data.frame(predict(dmy, newdata = df.numeric))
head(df.numeric)
```

```{r}
df.numeric$Survived<- as.factor(df.numeric$Survived)
levels(df.numeric$Survived)<-c("x0","x1")
```

Omit records that contain empty values:
```{r}
empty_records <- df.numeric[rowSums(is.na(df.numeric)) > 0,]
if (nrow(empty_records)>0)
  df.numeric<- na.omit(df.numeric)
```

Tide the data two times: the first is for categorial data and the second for numeric data.
```{r}
#install.packages("tidyr")
library(tidyr)
df_tidy_factors<-gather(df[,factors],"feature","value",-1)
df_tidy_numerics<-gather(cbind(Survived=df[,1],df[,numerics]),"feature","value",-1)
```


The first plot describes only categorical features (factors). 
Notice that the *scales* parameter was set to "free" to enable a suitable scaling for each facet (otherwise it is hard to view some of the facets, that need much smaller scales). We use the *facet_grid* that accepts a *scales* parameter.

```{r}
#install.packages("ggplot2")
library(ggplot2)
qplot(x=value,data=df_tidy_factors,fill=Survived) + facet_grid(~feature,scales="free")
```

One more plot for numeric features:
```{r}
qplot(x=value,data=df_tidy_numerics,fill=Survived) + facet_grid(~feature,scales="free")
```

It certainly looks like there are more chances to survive in certain levels of almost each feature.

Split the data into 80% train and 20% test sets.
```{r}
indices <- sample(1:nrow(df),nrow(df)*0.7)
train<- df[indices,]
test<- df[-indices,]
```

# Train a Random Forest model, evaluating it with ROC.
```{r}
set.seed(123)
fit.rf <- train(Survived~., data=df.numeric, method="rf",ntree = 10,  
              metric='ROC', 
              trControl=trainControl(method="cv", 
                                         number=5,
                                         savePredictions="final",
                                         summaryFunction = twoClassSummary,
                                         classProbs = TRUE),
              tuneGrid=expand.grid(.mtry=c(5)),
              na.action = na.pass)
```

Load the test file.
```{r}
new_df <-read.csv('Titanic/test.csv',na.strings = "")

summary(new_df)
```

Create a vector with the PassengerIds of records in the test file. We will soon attach it to the prediction results.
```{r}
ids<- new_df$PassengerId
```

Repeat the same preprocessing steps that were performed on the train data (factorizing two features and ignoring three features).
```{r}
new_df$Pclass<- as.factor(new_df$Pclass)
new_df<- new_df[,-c(1,3,8,10)]

new_df.numeric <- new_df
```

Use CARET's *dummyVars* to convert nominal features to various dummies or flags, according to each feature's possible levels.
```{r}
library(caret)
new_dmy <- dummyVars(" ~ .", data = new_df.numeric,fullRank=F)
new_df.numeric <- data.frame(predict(new_dmy, newdata = new_df.numeric))
head(new_df.numeric)
```

Omit records that contain empty values:
```{r}
empty_records <- new_df.numeric[rowSums(is.na(new_df.numeric)) > 0,]
if (nrow(empty_records)>0)
  new_df.numeric<- na.omit(new_df.numeric)
```

Load the test file and predict the target attribute for its records using your trained model. Don't forget to set *na.action* to "na.pass" if you used caret in the model creation, otherwise it will fail.
```{r}
new_pred<- predict(fit.rf,new_df.numeric,na.action = na.pass)
```


Write the *PassengerId* and *Survived* attributes to a csv file and submit this file to kaggle's competition (You should first subscribe to the site). What is your score?

```{r}
res <- cbind(PassengerId=ids,Survived=as.character(new_pred))
write.csv(res,file="Titanic/rf.csv",row.names = F)
```
